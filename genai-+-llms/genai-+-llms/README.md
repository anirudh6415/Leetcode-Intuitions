# GenAI + LLM's

### References:

### **ðŸ“Œ 1. Key NLP/LLM + GENAI Topics to Cover**

<table><thead><tr><th width="48.37823486328125" align="center">SI</th><th width="185.0369873046875" align="center">Section</th><th width="371.82672119140625" valign="middle">Topics to Cover</th><th width="64.334228515625" data-type="checkbox">Done</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">Word Embeddings </td><td valign="middle">Word2Vec, GloVe, FastText, contextual embeddings (ELMo, BERT)</td><td>false</td></tr><tr><td align="center">2</td><td align="center">NLP Tasks</td><td valign="middle">Classification, NER, POS tagging, QA, summarization, translation, etc.</td><td>false</td></tr><tr><td align="center">3</td><td align="center">Data processing</td><td valign="middle">Preprocessing, tokenization, stopword removal, stemming, lemmatization, data sampling</td><td>false</td></tr><tr><td align="center">4</td><td align="center">Architecture </td><td valign="middle">Neural, Attention, Transformers</td><td>false</td></tr><tr><td align="center">5</td><td align="center"> Token Sampling methods </td><td valign="middle">Greedy, Top-k, Top-p (nucleus), temperature, beam search</td><td>false</td></tr><tr><td align="center">6</td><td align="center">Encoder vs Decoder vs Encoder-decoder</td><td valign="middle">BERT, GPT, T5 architectures and use-cases</td><td>false</td></tr><tr><td align="center">7</td><td align="center">LLM's</td><td valign="middle">GPT series, PaLM, LLaMA, Claude, Gemini, Mistral, architecture overviews</td><td>false</td></tr><tr><td align="center">8</td><td align="center">Optimization</td><td valign="middle">Preference optimization, Reinforcement Learning with Human Feedback (RLHF), Proximal Policy Optimization (PPO)</td><td>false</td></tr><tr><td align="center">9</td><td align="center">Hallucinations </td><td valign="middle">Causes, detection methods, mitigation strategies</td><td>false</td></tr><tr><td align="center">10</td><td align="center">Machine Translation, Knowledge Graphs</td><td valign="middle">Seq2seq for MT, Transformer-based MT, embeddings into KG, using KG with LLMs</td><td>false</td></tr><tr><td align="center">11</td><td align="center">Retrieval Augmented Generation (RAG)</td><td valign="middle">Dense vs sparse retrieval, RAG pipelines, embedding stores, hybrid retrieval</td><td>false</td></tr><tr><td align="center">12</td><td align="center">Textual Entailment, LLM Context Length Extension</td><td valign="middle">NLI, entailment datasets (e.g., SNLI, MNLI), attention span, long-context models</td><td>false</td></tr><tr><td align="center">13</td><td align="center">Finetuning Methods </td><td valign="middle">Full fine-tuning, LoRA, PEFT, Adapter tuning, Prompt tuning, Instruction tuning</td><td>false</td></tr><tr><td align="center">14</td><td align="center">Prompt Engineering Techniques</td><td valign="middle">Zero-shot, few-shot, Chain-of-Thought, Self-consistency, ReAct, Tree of Thought</td><td>false</td></tr><tr><td align="center">15</td><td align="center">Evaluation of LLMs</td><td valign="middle">Human vs automatic metrics (BLEU, ROUGE, METEOR, perplexity), benchmarks (MMLU, HellaSwag, TruthfulQA, etc.</td><td>false</td></tr></tbody></table>

