# Cheat-sheet-ML-models

### Refreshers <a href="#linear-models-inanu" id="linear-models-inanu"></a>

{% embed url="https://stanford.edu/~shervine/teaching/cs-229/" %}

{% embed url="https://mlu-explain.github.io/" %}

### Linear Models <a href="#linear-models-inanu" id="linear-models-inanu"></a>

In a nutshell, linear models create a best-fit line to predict unseen data. Linear models imply that outputs are a linear combination of features. In this section, we'll specify commonly used linear models in machine learning, their advantages, and disadvantages.

<table data-header-hidden><thead><tr><th width="127.3193359375"></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td><strong>Algorithm</strong></td><td><strong>Description</strong></td><td><strong>Applications</strong></td><td><strong>Advantages</strong></td><td><strong>Disadvantages</strong></td></tr><tr><td><strong>Linear Regression</strong></td><td>A simple algorithm that models a linear relationship between inputs and a continuous numerical output variable</td><td><ol><li>Stock Price Prediction</li><li>Predicting housing prices</li><li>Predicting customer lifetime value</li></ol></td><td><ol><li>Explainable method</li><li>Interpretable results by its output coefficient</li><li>Faster to train than other machine learning models</li></ol></td><td><ol><li>Assumes linearity between inputs and output</li><li>Sensitive to outliers</li><li>Can underfit with small, high-dimensional data </li></ol></td></tr><tr><td><strong>Logistic Regression</strong></td><td>A simple algorithm that models a linear relationship between inputs and a categorical output (1 or 0)</td><td><ol><li>Predicting credit risk score</li><li>Customer churn prediction</li></ol></td><td><ol><li>Interpretable and explainable</li><li>Less prone to overfitting when using regularization</li><li>Applicable for multi-class predictions</li></ol></td><td><ol><li>Assumes linearity between inputs and outputs</li><li>Can overfit with small, high-dimensional data </li></ol></td></tr><tr><td><strong>Ridge Regression</strong></td><td>Part of the regression family — it penalizes features that have low predictive outcomes by shrinking their coefficients closer to zero. Can be used for classification or regression</td><td><ol><li>Predictive maintenance for automobiles</li><li>Sales revenue prediction</li></ol></td><td><ol><li>Less prone to overfitting</li><li>Best suited where data suffer from multicollinearity</li><li>Explainable &#x26; interpretable</li></ol></td><td><ol><li>All the predictors are kept in the final model</li><li>Doesn't perform feature selection</li></ol></td></tr><tr><td><strong>Lasso Regression</strong></td><td>Part of the regression family — it penalizes features that have low predictive outcomes by shrinking their coefficients to zero. Can be used for classification or regression</td><td><ol><li>Predicting housing prices</li><li>Predicting clinical outcomes based on health data</li></ol></td><td><ol><li>Less prone to overfitting</li><li>Can handle high-dimensional data</li><li>No need for feature selection</li></ol></td><td><ol><li>Can lead to poor interpretability as it can keep highly correlated variables</li></ol></td></tr></tbody></table>

### Tree-based models <a href="#tree-based-models-inanu" id="tree-based-models-inanu"></a>

In a nutshell, tree-based models use a series of "if-then" rules to predict from decision trees. In this section, we'll specify commonly used linear models in machine learning, their advantages, and disadvantages.

<table data-header-hidden><thead><tr><th width="130.54620361328125"></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td><strong>Algorithm</strong></td><td><strong>Description</strong></td><td><strong>Applications</strong></td><td><strong>Advantages</strong></td><td><strong>Disadvantages</strong></td></tr><tr><td><strong>Decision Tree</strong></td><td>Decision Tree models make decision rules on the features to produce predictions. It can be used for classification or regression</td><td><ol><li>Customer churn prediction</li><li>Credit score modeling</li><li>Disease prediction</li></ol></td><td><ol><li>Explainable and interpretable</li><li>Can handle missing values</li></ol></td><td><ol><li>Prone to overfitting</li><li>Sensitive to outliers</li></ol></td></tr><tr><td><strong>Random Forests</strong></td><td>An ensemble learning method that combines the output of multiple decision trees</td><td><ol><li>Credit score modeling</li><li>Predicting housing prices</li></ol></td><td><ol><li>Reduces overfitting</li><li>Higher accuracy compared to other models</li></ol></td><td><ol><li>Training complexity can be high</li><li>Not very interpretable</li></ol></td></tr><tr><td><strong>Gradient Boosting Regression</strong></td><td>Gradient Boosting Regression employs boosting to make predictive models from an ensemble of weak predictive learners</td><td><ol><li>Predicting car emissions</li><li>Predicting ride-hailing fare amount</li></ol></td><td><ol><li>Better accuracy compared to other regression models</li><li>It can handle multicollinearity<br>It can handle non-linear relationships</li></ol></td><td><ol><li>Sensitive to outliers and can therefore cause overfitting</li><li>Computationally expensive and has high complexity</li></ol></td></tr><tr><td><strong>XGBoost</strong></td><td>Gradient Boosting algorithm that is efficient &#x26; flexible. Can be used for both classification and regression tasks</td><td><ol><li>Churn prediction</li><li>Claims processing in insurance</li></ol></td><td><ol><li>Provides accurate results</li><li>Captures non-linear relationships</li></ol></td><td><ol><li>Hyperparameter tuning can be complex</li><li>Does not perform well on sparse datasets</li></ol></td></tr><tr><td><strong>LightGBM Regressor</strong></td><td>A gradient boosting framework that is designed to be more efficient than other implementations</td><td><ol><li>Predicting flight time for airlines</li><li>Predicting cholesterol levels based on health data</li></ol></td><td><ol><li>Can handle large amounts of data</li><li>Computational efficient &#x26; fast training speed</li><li>Low memory usage</li></ol></td><td><ol><li>Can overfit due to leaf-wise splitting and high sensitivity</li><li>Hyperparameter tuning can be complex</li></ol></td></tr></tbody></table>

### Unsupervised Learning <a href="#unsupervised-learning-unsup" id="unsupervised-learning-unsup"></a>

Unsupervised learning is about discovering general patterns in data. The most popular example is clustering or segmenting customers and users. This type of segmentation is generalizable and can be applied broadly, such as to documents, companies, and genes. Unsupervised learning consists of clustering models, that learn how to group similar data points together, or association algorithms, that group different data points based on pre-defined rules.&#x20;

#### Clustering models <a href="#clustering-models-less-than-tbo" id="clustering-models-less-than-tbo"></a>

| **Algorithm**               | **Description**                                                                                                                              | **Applications**                                                                  | **Advantages**                                                                                                                                                                 | **Disadvantages**                                                                                                                           |
| --------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |
| **K-Means**                 | K-Means is the most widely used clustering approach—it determines K clusters based on euclidean distances                                    | <ol><li>Customer segmentation</li><li>Recommendation systems</li></ol>            | <ol><li>Scales to large datasets</li><li>Simple to implement and interpret</li><li>Results in tight clusters</li></ol>                                                         | <ol><li>Requires the expected number of clusters from the beginning</li><li>Has troubles with varying cluster sizes and densities</li></ol> |
| **Hierarchical Clustering** | A "bottom-up" approach where each data point is treated as its own cluster—and then the closest two clusters are merged together iteratively | <ol><li>Fraud detection</li><li>Document clustering based on similarity</li></ol> | <ol><li>There is no need to specify the number of clusters</li><li>The resulting dendrogram is informative </li></ol>                                                          | <ol><li>Doesn’t always result in the best clustering</li><li>Not suitable for large datasets due to high complexity</li></ol>               |
| **Gaussian Mixture Models** | A probabilistic model for modeling normally distributed clusters within a dataset                                                            | <ol><li>Customer segmentation</li><li>Recommendation systems</li></ol>            | <ol><li>Computes a probability for an observation belonging to a cluster</li><li>Can identify overlapping clusters</li><li>More accurate results compared to K-means</li></ol> | <ol><li>Requires complex tuning</li><li>Requires setting the number of expected mixture components or clusters</li></ol>                    |

#### Association <a href="#association-and-nbsp" id="association-and-nbsp"></a>

| **Algorithm**         | **Description**                                                                                                                               | **Applications**                                                                                   | **Advantages**                                                                                                                                  | **Disadvantages**                                                                                                                              |
| --------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| **Apriori Algorithm** | Rule based approach that identifies the most frequent itemset in a given dataset where prior knowledge of frequent itemset properties is used | <ol><li>Product placements</li><li>Recommendation engines</li><li>Promotion optimization</li></ol> | <ol><li>Results are intuitive and Interpretable</li><li>Exhaustive approach as it finds all rules based on the confidence and support</li></ol> | <ol><li>Generates many uninteresting itemsets</li><li>Computationally and memory intensive.<br>Results in many overlapping item sets</li></ol> |

