# Machine Learning

### References:

{% embed url="https://mlu-explain.github.io/" %}

{% embed url="https://www.statlearning.com/" %}

{% embed url="https://stanford.edu/~shervine/teaching/cs-229/" %}

{% embed url="https://www.deep-ml.com/" %}
practice math and algorithms (similar to leet code)
{% endembed %}

{% embed url="https://dcai.csail.mit.edu/resources" %}

### **ðŸ“Œ 1. Key ML Topics to Cover**

<table><thead><tr><th width="48.37823486328125" align="center">SI</th><th width="185.0369873046875" align="center">Section</th><th width="371.82672119140625" valign="middle">Topics to Cover</th><th width="64.334228515625" data-type="checkbox">Done</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">ML Introduction</td><td valign="middle">What is ML? Difference between AI, ML, and DL, Types of ML (Supervised, Unsupervised, Reinforcement), Applications</td><td>true</td></tr><tr><td align="center">2</td><td align="center">Math for ML</td><td valign="middle">Linear Algebra (vectors, matrices, dot product), Calculus (gradients, chain rule), Probability &#x26; Statistics (mean, std, distributions, Bayes' Theorem)</td><td>true</td></tr><tr><td align="center">3</td><td align="center">Supervised Learning</td><td valign="middle">Linear Regression, Logistic Regression, Decision Trees, Random Forests, SVM, KNN, Naive Bayes, Gradient Boosting (XGBoost, LightGBM, CatBoost)</td><td>false</td></tr><tr><td align="center">4</td><td align="center">Unsupervised Learning</td><td valign="middle">K-Means, Hierarchical Clustering, DBSCAN, PCA, t-SNE, Anomaly Detection</td><td>false</td></tr><tr><td align="center">5</td><td align="center">Model Evaluation</td><td valign="middle">Confusion Matrix, Accuracy, Precision, Recall, F1 Score, ROC-AUC, Log Loss, RMSE, MAE, RÂ², Cross-Validation Techniques</td><td>false</td></tr><tr><td align="center">6</td><td align="center">Feature Engineering</td><td valign="middle">One-Hot/Label Encoding, Scaling (Standard, Min-Max), Binning, Polynomial Features, Feature Selection (RFE, Mutual Info), Missing Values, Outlier Detection</td><td>false</td></tr><tr><td align="center">7</td><td align="center">Model Optimization</td><td valign="middle">Regularization (L1, L2), Hyperparameter Tuning (Grid Search, Random Search, Bayesian), Early Stopping, Learning Curves</td><td>false</td></tr><tr><td align="center">8</td><td align="center">Training Techniques</td><td valign="middle">Batch vs Mini-Batch vs Stochastic Gradient Descent, Learning Rate Scheduling, Epochs vs Iterations, Class Imbalance Handling (SMOTE, class weights)</td><td>false</td></tr><tr><td align="center">9</td><td align="center">Explainability (XAI)</td><td valign="middle">SHAP, LIME, Feature Importance, Partial Dependence Plots (PDP), Global vs Local Explanations</td><td>false</td></tr><tr><td align="center">10</td><td align="center">Real-world Scenarios</td><td valign="middle">Model Drift, Concept Drift, Retraining Strategies, Monitoring, Data Leakage, Cold Start Problem, Interpretability vs Accuracy</td><td>false</td></tr><tr><td align="center">11</td><td align="center">Libraries &#x26; Tools</td><td valign="middle"><strong>scikit-learn</strong>, <strong>NumPy</strong>, <strong>Pandas</strong>, <strong>Matplotlib</strong>, <strong>Seaborn</strong>, <strong>XGBoost</strong>, <strong>LightGBM</strong>, <strong>CatBoost</strong>, <strong>SHAP</strong>, <strong>LIME</strong>, <strong>joblib</strong>, <strong>Pickle</strong>, <strong>TensorFlow/PyTorch</strong></td><td>false</td></tr><tr><td align="center">12</td><td align="center">Model Cheat sheet</td><td valign="middle">Classical ML model cheat sheet</td><td>true</td></tr></tbody></table>

